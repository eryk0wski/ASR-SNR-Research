{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from src.data_engineering import create_distribution_dict, creating_random_split_df, audio_normalizer\n",
    "from src.audio_mixer import mixer\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading already prepared audio and noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = creating_random_split_df(data['train'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating random splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing audio and noise to -20dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### AUDIO ####################\n",
    "\n",
    "#files_paths = df_audio['audiopath_local'].to_list()\n",
    "#file_names = df_audio['audiopath_bigos'].to_list()\n",
    "\n",
    "output_audio_folder_path = \"./data/demo/normalized_audio/\"\n",
    "\n",
    "#creating folder\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "for i in trange(len(files_paths)):\n",
    "    audio_normalizer(files_paths[i], output_folder_path,file_names[i], -20.0)\n",
    "\n",
    "\n",
    "#################### NOISE ##################3 \n",
    "    \n",
    "output_noise_folder_path = \"./data/demo/normalized_noise/\"\n",
    "\n",
    "#files_paths = df_audio['audiopath_local'].to_list()\n",
    "#file_names = df_audio['audiopath_bigos'].to_list()\n",
    "\n",
    "#creating folder\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "for i in trange(len(files_paths)):\n",
    "    audio_normalizer(files_paths[i], output_folder_path,file_names[i], -20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_values = [100,50,25,10,5,-1,-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each SNR value\n",
    "for snr in snr_values:\n",
    "    # Create a folder for the current SNR value\n",
    "    folder_path = f'./data/demo/normalized_audio/SNR_{snr}'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    # Loop through the dataframe and mix files for the current SNR value\n",
    "    snr_paths = []\n",
    "    for index, row in df_audio.iterrows():\n",
    "        signal_path = row['normalized_audio_path']\n",
    "        noise_path = row['normalized_noise_path']\n",
    "        audio_name = row['audiopath_bigos']\n",
    "        save_path = os.path.join(folder_path, audio_name)  # Change the naming convention if needed\n",
    "        snr_paths.append(save_path)\n",
    "\n",
    "        # Call your mixer function here\n",
    "        mixer(signal_path, noise_path, snr, save_path)\n",
    "    \n",
    "    column_name = f\"audio_SNR_{snr}_path\"\n",
    "    df_audio[column_name] = snr_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Checking if cuda is avalible\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#Checking if cuda is avalible\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Whisper v3 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the CUDA device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "torch_dtype = torch.float16  # You can adjust the dtype if needed\n",
    "\n",
    "# Load model and move it to CUDA\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create the pipeline with CUDA support\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = ['audio_SNR_100_path', 'audio_SNR_50_path', 'audio_SNR_25_path', 'audio_SNR_10_path', 'audio_SNR_5_path', 'audio_SNR_0.1_path', 'audio_SNR_-1_path', 'audio_SNR_-3_path', 'audio_SNR_-10_path']\n",
    "for snr in snr_list:\n",
    "    audio_paths = df_audio[snr].to_list()\n",
    "    results = []\n",
    "    for i in trange(len(audio_paths)):\n",
    "        sample = audio_paths[i]\n",
    "        result = pipe(sample, generate_kwargs={\"language\": \"polish\"})\n",
    "        results.append(result['text'])\n",
    "    col_name = f\"WER_{snr}\"\n",
    "    df_audio[col_name] = results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model jonatasgrosman/wav2vec2-large-xlsr-53-polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-polish\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch,column_name):\n",
    "    speech_array, sampling_rate = librosa.load(batch[column_name], sr=16_000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decrease Batch Size\n",
    "batch_size = 10\n",
    "\n",
    "# Gradient Accumulation\n",
    "accumulate_gradients = True\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
    "\n",
    "\n",
    "snr_paths = ['audio_SNR_100_path', 'audio_SNR_50_path', 'audio_SNR_25_path', 'audio_SNR_10_path', 'audio_SNR_5_path', 'audio_SNR_0.1_path', 'audio_SNR_-1_path', 'audio_SNR_-3_path', 'audio_SNR_-10_path']\n",
    "\n",
    "\n",
    "for snr in snr_paths:\n",
    "    test_dataset = ds.map(lambda batch: speech_file_to_array_fn(batch, snr))\n",
    "    predictions = []\n",
    "    num_batches = math.ceil(len(test_dataset) / batch_size)\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, len(test_dataset))\n",
    "        \n",
    "        inputs = processor(test_dataset[\"speech\"][batch_start:batch_end], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = inputs.to(device)\n",
    "        print('inputs done for:', snr)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "        print('torch done for:', snr)\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "        predictions.extend(predicted_sentences)\n",
    "        print('predictions appended')\n",
    "\n",
    "        # Gradient Accumulation\n",
    "        if accumulate_gradients and (i + 1) % gradient_accumulation_steps == 0:\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    col_name = f\"Wav2wec_{snr}\"\n",
    "    df_wav2wec[col_name] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASR_Techmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
